{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Pre-processing Text.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MYA9PC20JFsX",
        "outputId": "03186a23-7302-4b2d-dd81-ab076c4188e5"
      },
      "source": [
        "from google.colab import drive\r\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0nPtszJUnqYq"
      },
      "source": [
        "*Libraries and packages for Text-preprocessing.*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N4zvI2lbJP77",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6c7b62cc-050c-41f3-9a81-ad7bda6de79b"
      },
      "source": [
        "import pandas as pd\r\n",
        "import nltk          \r\n",
        "import string \r\n",
        "import re \r\n",
        "\r\n",
        "from nltk.tokenize import TweetTokenizer\r\n",
        "import nltk\r\n",
        "nltk.download('stopwords')\r\n",
        "from wordcloud import WordCloud,STOPWORDS\r\n",
        "nltk.download('wordnet')"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yf_JcYH8okni"
      },
      "source": [
        "*Read the csv file*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0MNjHkWcLOom"
      },
      "source": [
        "df = pd.read_csv('/content/data.csv', encoding= 'latin1')"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kSH_PNMaM-tE"
      },
      "source": [
        "   **DATETIME** \r\n",
        "\r\n",
        "  *   Deleting all rows that are not a 'datetime' type.\r\n",
        "\r\n",
        "  *   Seperating timestamp into date and time columns.\r\n",
        "\r\n",
        "\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lNOpP0cxMe0E"
      },
      "source": [
        "# Deleting all rows that are not a 'datetime' type\r\n",
        "df['tweet_timestamp'] = pd.to_datetime(df['tweet_timestamp'], errors='coerce')\r\n",
        "df = df.dropna(subset=['tweet_timestamp'])\r\n",
        "\r\n",
        "# Seperating timestamp into date and time columns\r\n",
        "df['tweet_timestamp'] = pd.to_datetime(df['tweet_timestamp']) \r\n",
        "df = df.sort_values(['tweet_timestamp'])\r\n",
        "df['date'] = df['tweet_timestamp'].astype(str).str.split(' ', expand=True)[0]\r\n",
        "df['time'] = df['tweet_timestamp'].astype(str).str.split(' ', expand=True)[1]\r\n"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a3L_C_MIo-bx"
      },
      "source": [
        "**TEXT PREPROCESSING FOE VADER SENTIMENT ANALYSIS**\r\n",
        "     (Five Heuristics that affect sentiment of a text.)\r\n",
        "\r\n",
        "---\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "1.   Punctuation (eg:'?','!') are allowed.\r\n",
        "2.   Capitalization ( eg:'I HATE YOU' is more negative than 'I hate you' )\r\n",
        "\r\n",
        "1.   Degree modifiers ( eg: 'Batting lineup is **extremely** good' vs. 'Batting lineup is good'. The first sentence is clearly more positive than the latter which affects the sentiment score.)\r\n",
        "2.   Constructive conjunction like 'but' shifts the polarity.\r\n",
        "\r\n",
        "1.   Trigram examination to identify negation.\r\n",
        "\r\n",
        "\r\n",
        "*   ' The food here isn't really that great'. Vader considers trigrams of words to identify negation i.e 'isn't really that', 'really that great' are some of the trigrams that will be formed. The first token containing isn't will negate the positive score in the next one.\r\n",
        "\r\n",
        "\r\n",
        "The cell below performs basic data cleaning tasks. To seperate noise from the text URL, hashtags, username and other special characters have been removed using regex library.\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qg8XOQWMvKsq"
      },
      "source": [
        "# remove whitespace from text \r\n",
        "def remove_whitespace(Full_text): \r\n",
        "    line = re.sub('[\\s]+', ' ', Full_text)\r\n",
        "    return line \r\n",
        "# remove url\r\n",
        "def remove_url(Full_text): \r\n",
        "    line = re.sub('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+','',Full_text)\r\n",
        "    line = re.sub(r\"www\\.[a-z]?\\.?(com)+|[a-z]+\\.(com)\", '', Full_text)\r\n",
        "    line = re.sub('https?://\\S+|www\\.\\S+', '', Full_text)\r\n",
        "    return line\r\n",
        "# Remove mentions\r\n",
        "def remove_mention(Full_text):\r\n",
        "    line=re.sub(r'@\\w+','',Full_text)\r\n",
        "    return line \r\n",
        "    \r\n",
        "# Remove hashtags\r\n",
        "#def remove_hash(Full_text):\r\n",
        "#    line=re.sub(r'#\\w+','',Full_text)\r\n",
        "#    return line\r\n",
        "\r\n",
        "# Remove numbers\r\n",
        "def remove_number(Full_text):\r\n",
        "    line=re.sub(r'[0-9]+','',Full_text)\r\n",
        "    return line\r\n",
        "# Remove punctuations(except \"?\" )\r\n",
        "def remove_punct(Full_text):\r\n",
        "    line = re.sub(r'[]!\"$%&\\'()*+,./:;=#@[\\\\^_`{|}~-]+', '', Full_text)\r\n",
        "    return line\r\n",
        "#\r\n",
        "def remove_thi_amp_ha_words(string):\r\n",
        "    line=re.sub(r'\\bamp\\b|\\bthi\\b|\\bha\\b','',string)\r\n",
        "    return line\r\n",
        "# Remove non-ascii characters\r\n",
        "def remove_non_ascii(Full_text):\r\n",
        "    \"\"\"\r\n",
        "        Remove non-ASCII characters \r\n",
        "    \"\"\"\r\n",
        "    return re.sub(r'[^\\x00-\\x7f]',r'', Full_text)"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jCtwG0R23k8D"
      },
      "source": [
        "VADER takes care of emojis and emoticons by converting them to their literal meaning. We have included this step to keep our input data same for K-means and VADER  approaches to evaluate sentiment."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sw2q345BkUcp"
      },
      "source": [
        "# Thanks : https://github.com/NeelShah18/emot/blob/master/emot/emo_unicode.py\r\n",
        "EMOTICONS = {\r\n",
        "    u\":‑\\)\":\"Happy face or smiley\",\r\n",
        "    u\":\\)\":\"Happy face or smiley\",\r\n",
        "    u\":-\\]\":\"Happy face or smiley\",\r\n",
        "    u\":\\]\":\"Happy face or smiley\",\r\n",
        "    u\":-3\":\"Happy face smiley\",\r\n",
        "    u\":3\":\"Happy face smiley\",\r\n",
        "    u\":->\":\"Happy face smiley\",\r\n",
        "    u\":>\":\"Happy face smiley\",\r\n",
        "    u\"8-\\)\":\"Happy face smiley\",\r\n",
        "    u\":o\\)\":\"Happy face smiley\",\r\n",
        "    u\":-\\}\":\"Happy face smiley\",\r\n",
        "    u\":\\}\":\"Happy face smiley\",\r\n",
        "    u\":-\\)\":\"Happy face smiley\",\r\n",
        "    u\":c\\)\":\"Happy face smiley\",\r\n",
        "    u\":\\^\\)\":\"Happy face smiley\",\r\n",
        "    u\"=\\]\":\"Happy face smiley\",\r\n",
        "    u\"=\\)\":\"Happy face smiley\",\r\n",
        "    u\":‑D\":\"Laughing, big grin or laugh with glasses\",\r\n",
        "    u\":D\":\"Laughing, big grin or laugh with glasses\",\r\n",
        "    u\"8‑D\":\"Laughing, big grin or laugh with glasses\",\r\n",
        "    u\"8D\":\"Laughing, big grin or laugh with glasses\",\r\n",
        "    u\"X‑D\":\"Laughing, big grin or laugh with glasses\",\r\n",
        "    u\"XD\":\"Laughing, big grin or laugh with glasses\",\r\n",
        "    u\"=D\":\"Laughing, big grin or laugh with glasses\",\r\n",
        "    u\"=3\":\"Laughing, big grin or laugh with glasses\",\r\n",
        "    u\"B\\^D\":\"Laughing, big grin or laugh with glasses\",\r\n",
        "    u\":-\\)\\)\":\"Very happy\",\r\n",
        "    u\":‑\\(\":\"Frown, sad, andry or pouting\",\r\n",
        "    u\":-\\(\":\"Frown, sad, andry or pouting\",\r\n",
        "    u\":\\(\":\"Frown, sad, andry or pouting\",\r\n",
        "    u\":‑c\":\"Frown, sad, andry or pouting\",\r\n",
        "    u\":c\":\"Frown, sad, andry or pouting\",\r\n",
        "    u\":‑<\":\"Frown, sad, andry or pouting\",\r\n",
        "    u\":<\":\"Frown, sad, andry or pouting\",\r\n",
        "    u\":‑\\[\":\"Frown, sad, andry or pouting\",\r\n",
        "    u\":\\[\":\"Frown, sad, andry or pouting\",\r\n",
        "    u\":-\\|\\|\":\"Frown, sad, andry or pouting\",\r\n",
        "    u\">:\\[\":\"Frown, sad, andry or pouting\",\r\n",
        "    u\":\\{\":\"Frown, sad, andry or pouting\",\r\n",
        "    u\":@\":\"Frown, sad, andry or pouting\",\r\n",
        "    u\">:\\(\":\"Frown, sad, andry or pouting\",\r\n",
        "    u\":'‑\\(\":\"Crying\",\r\n",
        "    u\":'\\(\":\"Crying\",\r\n",
        "    u\":'‑\\)\":\"Tears of happiness\",\r\n",
        "    u\":'\\)\":\"Tears of happiness\",\r\n",
        "    u\"D‑':\":\"Horror\",\r\n",
        "    u\"D:<\":\"Disgust\",\r\n",
        "    u\"D:\":\"Sadness\",\r\n",
        "    u\"D8\":\"Great dismay\",\r\n",
        "    u\"D;\":\"Great dismay\",\r\n",
        "    u\"D=\":\"Great dismay\",\r\n",
        "    u\"DX\":\"Great dismay\",\r\n",
        "    u\":‑O\":\"Surprise\",\r\n",
        "    u\":O\":\"Surprise\",\r\n",
        "    u\":‑o\":\"Surprise\",\r\n",
        "    u\":o\":\"Surprise\",\r\n",
        "    u\":-0\":\"Shock\",\r\n",
        "    u\"8‑0\":\"Yawn\",\r\n",
        "    u\">:O\":\"Yawn\",\r\n",
        "    u\":-\\*\":\"Kiss\",\r\n",
        "    u\":\\*\":\"Kiss\",\r\n",
        "    u\":X\":\"Kiss\",\r\n",
        "    u\";‑\\)\":\"Wink or smirk\",\r\n",
        "    u\";\\)\":\"Wink or smirk\",\r\n",
        "    u\"\\*-\\)\":\"Wink or smirk\",\r\n",
        "    u\"\\*\\)\":\"Wink or smirk\",\r\n",
        "    u\";‑\\]\":\"Wink or smirk\",\r\n",
        "    u\";\\]\":\"Wink or smirk\",\r\n",
        "    u\";\\^\\)\":\"Wink or smirk\",\r\n",
        "    u\":‑,\":\"Wink or smirk\",\r\n",
        "    u\";D\":\"Wink or smirk\",\r\n",
        "    u\":‑P\":\"Tongue sticking out, cheeky, playful or blowing a raspberry\",\r\n",
        "    u\":P\":\"Tongue sticking out, cheeky, playful or blowing a raspberry\",\r\n",
        "    u\"X‑P\":\"Tongue sticking out, cheeky, playful or blowing a raspberry\",\r\n",
        "    u\"XP\":\"Tongue sticking out, cheeky, playful or blowing a raspberry\",\r\n",
        "    u\":‑Þ\":\"Tongue sticking out, cheeky, playful or blowing a raspberry\",\r\n",
        "    u\":Þ\":\"Tongue sticking out, cheeky, playful or blowing a raspberry\",\r\n",
        "    u\":b\":\"Tongue sticking out, cheeky, playful or blowing a raspberry\",\r\n",
        "    u\"d:\":\"Tongue sticking out, cheeky, playful or blowing a raspberry\",\r\n",
        "    u\"=p\":\"Tongue sticking out, cheeky, playful or blowing a raspberry\",\r\n",
        "    u\">:P\":\"Tongue sticking out, cheeky, playful or blowing a raspberry\",\r\n",
        "    u\":‑/\":\"Skeptical, annoyed, undecided, uneasy or hesitant\",\r\n",
        "    u\":/\":\"Skeptical, annoyed, undecided, uneasy or hesitant\",\r\n",
        "    u\":-[.]\":\"Skeptical, annoyed, undecided, uneasy or hesitant\",\r\n",
        "    u\">:[(\\\\\\)]\":\"Skeptical, annoyed, undecided, uneasy or hesitant\",\r\n",
        "    u\">:/\":\"Skeptical, annoyed, undecided, uneasy or hesitant\",\r\n",
        "    u\":[(\\\\\\)]\":\"Skeptical, annoyed, undecided, uneasy or hesitant\",\r\n",
        "    u\"=/\":\"Skeptical, annoyed, undecided, uneasy or hesitant\",\r\n",
        "    u\"=[(\\\\\\)]\":\"Skeptical, annoyed, undecided, uneasy or hesitant\",\r\n",
        "    u\":L\":\"Skeptical, annoyed, undecided, uneasy or hesitant\",\r\n",
        "    u\"=L\":\"Skeptical, annoyed, undecided, uneasy or hesitant\",\r\n",
        "    u\":S\":\"Skeptical, annoyed, undecided, uneasy or hesitant\",\r\n",
        "    u\":‑\\|\":\"Straight face\",\r\n",
        "    u\":\\|\":\"Straight face\",\r\n",
        "    u\":$\":\"Embarrassed or blushing\",\r\n",
        "    u\":‑x\":\"Sealed lips or wearing braces or tongue-tied\",\r\n",
        "    u\":x\":\"Sealed lips or wearing braces or tongue-tied\",\r\n",
        "    u\":‑#\":\"Sealed lips or wearing braces or tongue-tied\",\r\n",
        "    u\":#\":\"Sealed lips or wearing braces or tongue-tied\",\r\n",
        "    u\":‑&\":\"Sealed lips or wearing braces or tongue-tied\",\r\n",
        "    u\":&\":\"Sealed lips or wearing braces or tongue-tied\",\r\n",
        "    u\"O:‑\\)\":\"Angel, saint or innocent\",\r\n",
        "    u\"O:\\)\":\"Angel, saint or innocent\",\r\n",
        "    u\"0:‑3\":\"Angel, saint or innocent\",\r\n",
        "    u\"0:3\":\"Angel, saint or innocent\",\r\n",
        "    u\"0:‑\\)\":\"Angel, saint or innocent\",\r\n",
        "    u\"0:\\)\":\"Angel, saint or innocent\",\r\n",
        "    u\":‑b\":\"Tongue sticking out, cheeky, playful or blowing a raspberry\",\r\n",
        "    u\"0;\\^\\)\":\"Angel, saint or innocent\",\r\n",
        "    u\">:‑\\)\":\"Evil or devilish\",\r\n",
        "    u\">:\\)\":\"Evil or devilish\",\r\n",
        "    u\"\\}:‑\\)\":\"Evil or devilish\",\r\n",
        "    u\"\\}:\\)\":\"Evil or devilish\",\r\n",
        "    u\"3:‑\\)\":\"Evil or devilish\",\r\n",
        "    u\"3:\\)\":\"Evil or devilish\",\r\n",
        "    u\">;\\)\":\"Evil or devilish\",\r\n",
        "    u\"\\|;‑\\)\":\"Cool\",\r\n",
        "    u\"\\|‑O\":\"Bored\",\r\n",
        "    u\":‑J\":\"Tongue-in-cheek\",\r\n",
        "    u\"#‑\\)\":\"Party all night\",\r\n",
        "    u\"%‑\\)\":\"Drunk or confused\",\r\n",
        "    u\"%\\)\":\"Drunk or confused\",\r\n",
        "    u\":-###..\":\"Being sick\",\r\n",
        "    u\":###..\":\"Being sick\",\r\n",
        "    u\"<:‑\\|\":\"Dump\",\r\n",
        "    u\"\\(>_<\\)\":\"Troubled\",\r\n",
        "    u\"\\(>_<\\)>\":\"Troubled\",\r\n",
        "    u\"\\(';'\\)\":\"Baby\",\r\n",
        "    u\"\\(\\^\\^>``\":\"Nervous or Embarrassed or Troubled or Shy or Sweat drop\",\r\n",
        "    u\"\\(\\^_\\^;\\)\":\"Nervous or Embarrassed or Troubled or Shy or Sweat drop\",\r\n",
        "    u\"\\(-_-;\\)\":\"Nervous or Embarrassed or Troubled or Shy or Sweat drop\",\r\n",
        "    u\"\\(~_~;\\) \\(・\\.・;\\)\":\"Nervous or Embarrassed or Troubled or Shy or Sweat drop\",\r\n",
        "    u\"\\(-_-\\)zzz\":\"Sleeping\",\r\n",
        "    u\"\\(\\^_-\\)\":\"Wink\",\r\n",
        "    u\"\\(\\(\\+_\\+\\)\\)\":\"Confused\",\r\n",
        "    u\"\\(\\+o\\+\\)\":\"Confused\",\r\n",
        "    u\"\\(o\\|o\\)\":\"Ultraman\",\r\n",
        "    u\"\\^_\\^\":\"Joyful\",\r\n",
        "    u\"\\(\\^_\\^\\)/\":\"Joyful\",\r\n",
        "    u\"\\(\\^O\\^\\)／\":\"Joyful\",\r\n",
        "    u\"\\(\\^o\\^\\)／\":\"Joyful\",\r\n",
        "    u\"\\(__\\)\":\"Kowtow as a sign of respect, or dogeza for apology\",\r\n",
        "    u\"_\\(\\._\\.\\)_\":\"Kowtow as a sign of respect, or dogeza for apology\",\r\n",
        "    u\"<\\(_ _\\)>\":\"Kowtow as a sign of respect, or dogeza for apology\",\r\n",
        "    u\"<m\\(__\\)m>\":\"Kowtow as a sign of respect, or dogeza for apology\",\r\n",
        "    u\"m\\(__\\)m\":\"Kowtow as a sign of respect, or dogeza for apology\",\r\n",
        "    u\"m\\(_ _\\)m\":\"Kowtow as a sign of respect, or dogeza for apology\",\r\n",
        "    u\"\\('_'\\)\":\"Sad or Crying\",\r\n",
        "    u\"\\(/_;\\)\":\"Sad or Crying\",\r\n",
        "    u\"\\(T_T\\) \\(;_;\\)\":\"Sad or Crying\",\r\n",
        "    u\"\\(;_;\":\"Sad of Crying\",\r\n",
        "    u\"\\(;_:\\)\":\"Sad or Crying\",\r\n",
        "    u\"\\(;O;\\)\":\"Sad or Crying\",\r\n",
        "    u\"\\(:_;\\)\":\"Sad or Crying\",\r\n",
        "    u\"\\(ToT\\)\":\"Sad or Crying\",\r\n",
        "    u\";_;\":\"Sad or Crying\",\r\n",
        "    u\";-;\":\"Sad or Crying\",\r\n",
        "    u\";n;\":\"Sad or Crying\",\r\n",
        "    u\";;\":\"Sad or Crying\",\r\n",
        "    u\"Q\\.Q\":\"Sad or Crying\",\r\n",
        "    u\"T\\.T\":\"Sad or Crying\",\r\n",
        "    u\"QQ\":\"Sad or Crying\",\r\n",
        "    u\"Q_Q\":\"Sad or Crying\",\r\n",
        "    u\"\\(-\\.-\\)\":\"Shame\",\r\n",
        "    u\"\\(-_-\\)\":\"Shame\",\r\n",
        "    u\"\\(一一\\)\":\"Shame\",\r\n",
        "    u\"\\(；一_一\\)\":\"Shame\",\r\n",
        "    u\"\\(=_=\\)\":\"Tired\",\r\n",
        "    u\"\\(=\\^\\·\\^=\\)\":\"cat\",\r\n",
        "    u\"\\(=\\^\\·\\·\\^=\\)\":\"cat\",\r\n",
        "    u\"=_\\^=\t\":\"cat\",\r\n",
        "    u\"\\(\\.\\.\\)\":\"Looking down\",\r\n",
        "    u\"\\(\\._\\.\\)\":\"Looking down\",\r\n",
        "    u\"\\^m\\^\":\"Giggling with hand covering mouth\",\r\n",
        "    u\"\\(\\・\\・?\":\"Confusion\",\r\n",
        "    u\"\\(?_?\\)\":\"Confusion\",\r\n",
        "    u\">\\^_\\^<\":\"Normal Laugh\",\r\n",
        "    u\"<\\^!\\^>\":\"Normal Laugh\",\r\n",
        "    u\"\\^/\\^\":\"Normal Laugh\",\r\n",
        "    u\"\\（\\*\\^_\\^\\*）\" :\"Normal Laugh\",\r\n",
        "    u\"\\(\\^<\\^\\) \\(\\^\\.\\^\\)\":\"Normal Laugh\",\r\n",
        "    u\"\\(^\\^\\)\":\"Normal Laugh\",\r\n",
        "    u\"\\(\\^\\.\\^\\)\":\"Normal Laugh\",\r\n",
        "    u\"\\(\\^_\\^\\.\\)\":\"Normal Laugh\",\r\n",
        "    u\"\\(\\^_\\^\\)\":\"Normal Laugh\",\r\n",
        "    u\"\\(\\^\\^\\)\":\"Normal Laugh\",\r\n",
        "    u\"\\(\\^J\\^\\)\":\"Normal Laugh\",\r\n",
        "    u\"\\(\\*\\^\\.\\^\\*\\)\":\"Normal Laugh\",\r\n",
        "    u\"\\(\\^—\\^\\）\":\"Normal Laugh\",\r\n",
        "    u\"\\(#\\^\\.\\^#\\)\":\"Normal Laugh\",\r\n",
        "    u\"\\（\\^—\\^\\）\":\"Waving\",\r\n",
        "    u\"\\(;_;\\)/~~~\":\"Waving\",\r\n",
        "    u\"\\(\\^\\.\\^\\)/~~~\":\"Waving\",\r\n",
        "    u\"\\(-_-\\)/~~~ \\($\\·\\·\\)/~~~\":\"Waving\",\r\n",
        "    u\"\\(T_T\\)/~~~\":\"Waving\",\r\n",
        "    u\"\\(ToT\\)/~~~\":\"Waving\",\r\n",
        "    u\"\\(\\*\\^0\\^\\*\\)\":\"Excited\",\r\n",
        "    u\"\\(\\*_\\*\\)\":\"Amazed\",\r\n",
        "    u\"\\(\\*_\\*;\":\"Amazed\",\r\n",
        "    u\"\\(\\+_\\+\\) \\(@_@\\)\":\"Amazed\",\r\n",
        "    u\"\\(\\*\\^\\^\\)v\":\"Laughing,Cheerful\",\r\n",
        "    u\"\\(\\^_\\^\\)v\":\"Laughing,Cheerful\",\r\n",
        "    u\"\\(\\(d[-_-]b\\)\\)\":\"Headphones,Listening to music\",\r\n",
        "    u'\\(-\"-\\)':\"Worried\",\r\n",
        "    u\"\\(ーー;\\)\":\"Worried\",\r\n",
        "    u\"\\(\\^0_0\\^\\)\":\"Eyeglasses\",\r\n",
        "    u\"\\(\\＾ｖ\\＾\\)\":\"Happy\",\r\n",
        "    u\"\\(\\＾ｕ\\＾\\)\":\"Happy\",\r\n",
        "    u\"\\(\\^\\)o\\(\\^\\)\":\"Happy\",\r\n",
        "    u\"\\(\\^O\\^\\)\":\"Happy\",\r\n",
        "    u\"\\(\\^o\\^\\)\":\"Happy\",\r\n",
        "    u\"\\)\\^o\\^\\(\":\"Happy\",\r\n",
        "    u\":O o_O\":\"Surprised\",\r\n",
        "    u\"o_0\":\"Surprised\",\r\n",
        "    u\"o\\.O\":\"Surpised\",\r\n",
        "    u\"\\(o\\.o\\)\":\"Surprised\",\r\n",
        "    u\"oO\":\"Surprised\",\r\n",
        "    u\"\\(\\*￣m￣\\)\":\"Dissatisfied\",\r\n",
        "    u\"\\(‘A`\\)\":\"Snubbed or Deflated\"\r\n",
        "}\r\n",
        "# Convert emoticons to words\r\n",
        "def convert_emoticons(text):\r\n",
        "    for emot in EMOTICONS:\r\n",
        "        text = re.sub(u'('+emot+')', \"_\".join(EMOTICONS[emot].replace(\",\",\"\").split()), text)\r\n",
        "    return text"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aQdYapr34d0g"
      },
      "source": [
        "Applying the text cleaning functions on the text data.\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Lo7SkJOijwv"
      },
      "source": [
        "df['Full_text']=df['Full_text'].apply(lambda x:remove_whitespace(x))\r\n",
        "df['Full_text']=df['Full_text'].apply(lambda x:convert_emoticons(x))\r\n",
        "df['Full_text']=df['Full_text'].apply(lambda x:remove_mention(str(x)))\r\n",
        "#df['Full_text']=df['Full_text'].apply(lambda x:remove_hash(str(x)))\r\n",
        "df['Full_text']=df['Full_text'].apply(lambda x:remove_url(x))\r\n",
        "df['Full_text']=df['Full_text'].apply(lambda x:remove_number(x))\r\n",
        "df['Full_text']=df['Full_text'].apply(lambda x:remove_punct(x))\r\n",
        "df['Full_text']=df['Full_text'].apply(lambda x:remove_thi_amp_ha_words(x))\r\n",
        "df['Full_text']=df['Full_text'].apply(lambda x:remove_non_ascii(x))\r\n"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9ujf9Irl6uBJ"
      },
      "source": [
        "Tokenizing the cleaned data using a special tweet tokenizer instead of word tokenizer. Both works in the same way, to split a sentence into words just that former keeps the hashtags intact."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lX78TO2Aisvz"
      },
      "source": [
        "from nltk.tokenize import TweetTokenizer\r\n",
        "tknzr = TweetTokenizer()\r\n",
        "df['tokens'] = df['Full_text'].apply(tknzr.tokenize)"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-miccwhX7b0p"
      },
      "source": [
        "\r\n",
        "\r\n",
        "*    Stopwords like 'a', 'the',  'we' etc. are very common and doesn't \r\n",
        " contribute to the sentiment score.\r\n",
        "*   Some other common words populating our corpus have also been added to the STOPWORDS library.\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EDbY7XhJjIRO"
      },
      "source": [
        "stopwords = nltk.corpus.stopwords.words('english')\r\n",
        "# Adding irrelevant words in our STOPWORDS library\r\n",
        "newStopWords = ['played','playing','u','A','today','if','th','would','To','dont','v','run','st','go','match','test','team','Test','I','series','cricket','The','Team','Th']\r\n",
        "stopwords.extend(newStopWords)\r\n",
        "\r\n",
        "def remove_stopwords(text):\r\n",
        "    text = [word for word in text if word not in stopwords]\r\n",
        "    return text\r\n",
        "    \r\n",
        "df['No_stopwords'] = df['tokens'].apply(lambda x: remove_stopwords(x))"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TIPwBc7f9TL5"
      },
      "source": [
        "**Lemmatizing the tokens**\r\n",
        "\r\n",
        "\r\n",
        "*   Lemmatizer function reduces the word to its root form using its POS tag.\r\n",
        "*   Eg: Running, walking,swimming to run, walk,swim.\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F6v3RKTAlHL2"
      },
      "source": [
        "lemmatizer = nltk.WordNetLemmatizer()\r\n",
        "\r\n",
        "def lemmatize_words(text):\r\n",
        "    text = [lemmatizer.lemmatize(word) for word in text]\r\n",
        "    return text\r\n",
        "\r\n",
        "df['lemmatized'] = df['No_stopwords'].apply(lambda x: lemmatize_words(x))"
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NDgj4lBWAzRV"
      },
      "source": [
        "Cell below join the tokens in the lemmatized list of words and subsequently extract it from the list as a single item string in column text_modified."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HXB5qsQYlNid"
      },
      "source": [
        "def join(text):\r\n",
        "    text = [' '.join(str(j) for j in text)]\r\n",
        "    return text\r\n",
        "\r\n",
        "df['joined_string'] = df['lemmatized'].apply(lambda x: join(x))\r\n",
        "df['text_modified'] =  df['joined_string'].apply(lambda x: x[0])"
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cECB7rOflaN4",
        "outputId": "00198b5c-4c83-46d6-d9c9-84b6b29c7d3f"
      },
      "source": [
        "# Most frequent words      \r\n",
        "from collections import Counter\r\n",
        "cnt = Counter()\r\n",
        "for text in df[\"text_modified\"].values:\r\n",
        "    for word in text.split():\r\n",
        "        cnt[word] += 1\r\n",
        "\r\n",
        "# Removing most frequent words\r\n",
        "#FREQWORDS = set([w for (w, wc) in cnt.most_common(10)])\r\n",
        "#def remove_freqwords(text):\r\n",
        "   #    return \" \".join([word for word in str(text).split() if word not in FREQWORDS])\r\n",
        "        \r\n",
        "cnt.most_common(10)\r\n",
        "# Removing rare words\r\n",
        "n_rare_words = 10\r\n",
        "RAREWORDS = set([w for (w, wc) in cnt.most_common()[:-n_rare_words-1:-1]])\r\n",
        "def remove_rarewords(text):\r\n",
        "    return \" \".join([word for word in str(text).split() if word not in RAREWORDS])\r\n",
        "\r\n",
        "df['text_modified']=df['text_modified'].apply(lambda x:remove_rarewords(x))\r\n",
        "cnt.most_common(10)"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('India', 1843),\n",
              " ('?', 1143),\n",
              " ('Australia', 836),\n",
              " ('Indian', 829),\n",
              " ('win', 771),\n",
              " ('day', 717),\n",
              " ('Gabba', 543),\n",
              " ('player', 535),\n",
              " ('one', 493),\n",
              " ('like', 472)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JQR8StBpmzK-"
      },
      "source": [
        "df.to_csv(r'C:\\Users\\91987\\Desktop\\Twitter Sentiment Analysis\\File Name.csv', index = False)"
      ],
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KWnbcjpOFnDv"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}